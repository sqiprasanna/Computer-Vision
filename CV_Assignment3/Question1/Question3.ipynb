{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ResNet18.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2, numpy as np, os\n",
    "import pandas as pd, time\n",
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "decayOfBN = 0.0001\n",
    "epsOfBN = 1e-05\n",
    "\n",
    "\n",
    "\n",
    "def getMinibatches(feature, feature_label, crop = True, batch_size = 32):\n",
    "    \n",
    "    batch_n = feature.shape[0] // batch_size\n",
    "    data = list(zip(feature, feature_label))\n",
    "    np.random.shuffle(data)\n",
    "    feature, feature_label = zip(*data)\n",
    "    feature = np.array(feature)\n",
    "    feature_label = np.array(feature_label)\n",
    "    mini_batches = []\n",
    "    i = 0\n",
    "    for i in range(batch_n):\n",
    "        feature_mini = feature[i * batch_size:(i + 1)*batch_size]\n",
    "        if(crop):\n",
    "            feature_mini = DataAugmentation(feature_mini, 224)\n",
    "        feature_label_mini = feature_label[i * batch_size:(i + 1)*batch_size]\n",
    "        mini_batches.append((feature_mini, feature_label_mini))\n",
    "    if(feature.shape[0] % batch_size != 0):\n",
    "        feature_mini = feature[i * batch_size:feature.shape[0]]\n",
    "        if(crop):\n",
    "            feature_mini = DataAugmentation(feature_mini, 224)\n",
    "        feature_label_mini = feature_label[i * batch_size:feature_label.shape[0]]\n",
    "        mini_batches.append((feature_mini, feature_label_mini))\n",
    "        batch_n += 1\n",
    "    return mini_batches, batch_n\n",
    "\n",
    "def RandCrop(batch, crop_shape, padding=None):\n",
    "    \n",
    "    i_shape = np.shape(batch[0])\n",
    "    if padding:\n",
    "        i_shape = (i_shape[0] + 2 * padding, i_shape[1] + 2 * padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode='constant', constant_values=0)\n",
    "        nh = np.random.randint(0, i_shape[0] - crop_shape[0])\n",
    "        nw = np.random.randint(0, i_shape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "        \n",
    "    \n",
    "def DataAugmentation(batch, img_size):\n",
    "    return RandCrop(batch, [img_size, img_size, 3])\n",
    "\n",
    "def ReadImages(path, ran, phase):\n",
    "    save_path = phase+\"_images.npy\"\n",
    "    if(os.path.efeatureists(save_path)):\n",
    "        return np.load(save_path)\n",
    "    images = []\n",
    "    for r, d, f in os.walk(path):\n",
    "        for i in range(1, ran+1):\n",
    "            img_name = os.path.join(r, str(i)+\".jpg\")\n",
    "            images.append(cv2.imread(img_name)/255.)\n",
    "    if(phase == 'test'):\n",
    "        images = DataAugmentation(images, 224)\n",
    "    print(\"done loading from \" + path)\n",
    "    images = np.array(images)\n",
    "    np.save(save_path, images)\n",
    "    return images\n",
    "\n",
    "def ReadLabels(csv_file, phase, nb_classes = 8):\n",
    "    \n",
    "    labels = np.array(pd.read_csv(csv_file, header = None).values)\n",
    "    labels = np.reshape(labels, (labels.shape[1], labels.shape[0]))\n",
    "    one_hot_targets = np.eye(nb_classes)[[i-1 for i in labels]]\n",
    "    one_hot_targets = np.reshape(one_hot_targets, (one_hot_targets.shape[0], 8))\n",
    "\n",
    "    return one_hot_targets\n",
    "    \n",
    "\n",
    "def get_num_trainable_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_images = ReadImages(\"/content/drive/My Drive/Colab Notebooks/CVass2/train/\", 1888, \"train\")\n",
    "test_images = ReadImages(\"/content/drive/My Drive/Colab Notebooks/CVass2/test/\", 800, \"test\")\n",
    "\n",
    "train_labels = ReadLabels(\"/content/drive/My Drive/Colab Notebooks/CVass2/train_labels.csv\", \"train\")\n",
    "test_labels = ReadLabels(\"/content/drive/My Drive/Colab Notebooks/CVass2/test_labels.csv\", \"test\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resBlock(feature, filter_size, wt_init, weight_regularizer, conv_number, instance, is_train, stride = 1, reuse = False):\n",
    "    prev = feature\n",
    "      \n",
    "    feature = tf.layers.conv2d(feature, filters=filter_size, kernel_size=3, kernel_initializer=wt_init, strides=stride,\n",
    "                                kernel_regularizer=weight_regularizer, padding=\"SAME\", trainable=is_train, name=\"shortcut_conv\"+conv_number+\"_\"+instance, reuse=reuse)\n",
    "    feature = tf_contrib.layers.batch_norm(feature, decay=decayOfBN, epsilon=epsOfBN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"short_batch_norm\"+conv_number+\"_\"+instance)\n",
    "    feature = tf.nn.relu(feature)\n",
    "    feature = tf.layers.conv2d(feature, filters=filter_size, kernel_size=3, kernel_initializer=wt_init, trainable=is_train,\n",
    "                         kernel_regularizer=weight_regularizer, name=\"conv\"+conv_number+\"_\"+instance, padding=\"SAME\", reuse=reuse)\n",
    "    \n",
    "    feature = tf_contrib.layers.batch_norm(feature, decay=decayOfBN, epsilon=epsOfBN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"batch_norm\"+conv_number+\"_\"+instance)\n",
    "    \n",
    "    if(stride == 2):\n",
    "        prev = tf.layers.conv2d(prev, filters=filter_size, kernel_size=1, kernel_initializer=wt_init, strides=stride,\n",
    "                                kernel_regularizer=weight_regularizer, padding=\"SAME\", trainable=is_train, name=\"1feature1_\"+conv_number+\"_\"+instance, reuse=reuse)\n",
    "    \n",
    "    feature = prev + feature\n",
    "    feature = tf.nn.relu(feature)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def BuildModel(train_input, is_train = False, reuse = False):\n",
    "    wt_init = tf_contrib.layers.featureavier_initializer()\n",
    "    weight_regularizer = tf_contrib.layers.l2_regularizer(0.0001)\n",
    "    \n",
    "    feature = tf.layers.conv2d(train_input, filters=64, kernel_size=7, kernel_initializer=wt_init, strides=2, \n",
    "                         kernel_regularizer=weight_regularizer, trainable=is_train, name=\"conv1\", padding='SAME', reuse=reuse)\n",
    "    feature = tf_contrib.layers.batch_norm(feature, decay=decayOfBN, epsilon=epsOfBN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"batch_norm1\")\n",
    "    feature = tf.nn.relu(feature)\n",
    "    \n",
    "    feature = tf.layers.mafeature_pooling2d(feature, pool_size = 2, strides = 2, name=\"mafeaturepool\") # 56feature56feature64\n",
    "    \n",
    "    feature = resBlock(feature, 64, wt_init, weight_regularizer, \"2\", \"1\", is_train, reuse=reuse)\n",
    "    feature = resBlock(feature, 64, wt_init, weight_regularizer, \"2\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    feature = resBlock(feature, 128, wt_init, weight_regularizer, \"3\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    feature = resBlock(feature, 128, wt_init, weight_regularizer, \"3\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    feature = resBlock(feature, 256, wt_init, weight_regularizer, \"4\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    feature = resBlock(feature, 256, wt_init, weight_regularizer, \"4\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    feature = resBlock(feature, 512, wt_init, weight_regularizer, \"5\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    feature = resBlock(feature, 512, wt_init, weight_regularizer, \"5\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    feature = tf.layers.average_pooling2d(feature, pool_size=7, strides=2, name=\"avgpool\")\n",
    "    feature = tf.contrib.layers.flatten(feature)\n",
    "     \n",
    "    feature = tf.layers.dense(feature, 8, kernel_initializer=wt_init, trainable=is_train, name=\"output\", reuse=reuse)  \n",
    "    return feature\n",
    "\n",
    "def GetLosses(output, label):\n",
    "    loss = tf.reduce_mean(tf.nn.softmafeature_cross_entropy_with_logits_v2(logits=output, labels=label))\n",
    "    prediction = tf.equal(tf.argmafeature(output, -1), tf.argmafeature(label, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_input = tf.placeholder(tf.float32, (None, 224, 224, 3), name = \"train_input\")\n",
    "train_label = tf.placeholder(tf.int32, (None, 8), name = \"train_label\")\n",
    "test_input = tf.placeholder(tf.float32, (None, 224, 224, 3), name = \"test_input\")\n",
    "test_label = tf.placeholder(tf.int32, (None, 8), name = \"test_label\")\n",
    "\n",
    "batch = 32\n",
    "\n",
    "lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "train_output = BuildModel(train_input, is_train = True)\n",
    "test_output = BuildModel(test_input, reuse = True)\n",
    "\n",
    "train_loss, train_acc = GetLosses(train_output, train_label)\n",
    "test_loss, test_acc = GetLosses(test_output, test_label)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(train_loss)\n",
    "\n",
    "\n",
    "summary_train_loss = tf.summary.scalar(\"train_loss\", train_loss)\n",
    "summary_test_loss = tf.summary.scalar(\"train_loss\", test_loss)\n",
    "\n",
    "summary_train_accuracy = tf.summary.scalar(\"train_accuracy\", train_acc)\n",
    "summary_test_accuracy = tf.summary.scalar(\"train_accuracy\", test_acc)\n",
    "\n",
    "train_summary = tf.summary.merge([summary_train_loss, summary_train_accuracy])\n",
    "test_summary = tf.summary.merge([summary_test_loss, summary_test_accuracy])\n",
    "\n",
    "\n",
    "epo = 40\n",
    "epo_n = 0.1\n",
    "cntr = 0\n",
    "test_cntr = 0\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Starting Train!!!\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "for each_epo in range(epo):\n",
    "    if each_epo == int(epo * 0.5) or each_epo == int(epo * 0.75):\n",
    "        epo_n = epo_n * 0.1\n",
    "    each_minibatches, n_batches = getMinibatches(train_images, train_labels, batch_size = batch)\n",
    "    i = 0\n",
    "    for each_minibatch in each_minibatches:\n",
    "        i += 1\n",
    "        feature, y = each_minibatch\n",
    "        \n",
    "        train_feed_dict = {train_input:feature, train_label: y, lr: epo_n}\n",
    "        \n",
    "        acc, _, l, summary1 = sess.run([train_acc, optimizer, train_loss, train_summary], feed_dict=train_feed_dict)\n",
    "        writer.add_summary(summary1, cntr)\n",
    "        cntr += 1\n",
    "        \n",
    "        print(\"each_epo: [%2d] [%5d/%5d], train_accuracy: %.2f, learning_rate : %.4f, train_loss: %.2f\"                       % (each_epo, i, n_batches, acc, epo_n, l))\n",
    "    if(each_epo%5 == 0):\n",
    "        if not os.path.efeatureists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver.save(sess, os.path.join(checkpoint_dir, 'ResNet18.model'), global_step=cntr)\n",
    "        \n",
    "    each_minibatches, n_batches = getMinibatches(test_images, test_labels, batch_size = batch, crop=False)\n",
    "    for each_minibatch in each_minibatches:\n",
    "        feature, y = each_minibatch\n",
    "        \n",
    "        test_feed_dict = {test_input:feature, test_label: y}\n",
    "        \n",
    "        t_acc, summary2, t_loss = sess.run([test_acc, test_summary, test_loss], feed_dict=test_feed_dict)\n",
    "        \n",
    "        test_accs.append(t_acc)\n",
    "        test_losses.append(t_loss)\n",
    "        writer.add_summary(summary2, test_cntr)\n",
    "        print(\"each_epo: [%2d], test_accuracy: %.2f, test_loss: %.2f\"%(each_epo, t_acc, t_loss))\n",
    "        test_cntr += 1\n",
    "    \n",
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
